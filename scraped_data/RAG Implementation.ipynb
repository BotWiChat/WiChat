{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyP7kcphYD3wJudy3QnSr5/i"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# RAG Implementation\n","\n","This notebook contains the code for the implemenation of the Retrrieval Augmented Generation feature for the Worldbank IDEAS Chatbot (WiChat).\n","\n","## Step 1: Data Ingestion\n","Our data sources are 79 .txt files scraped from the Wordldbank Ideas Project Website and Worldbank Ideas Project Social Media. We can use their social media because it is publicly available and not private which we would have needed permission to use. Also, only websites allowed for crawling as detailed on the sitemap were scraped. View the scraping of the data here.\n","\n","langchain for framework\n","\n","HuggingFace (minilm) for embedding\n","\n","Falcon 7b instruct as LLM\n","\n","FAISS as vectore store (knowledge base)"],"metadata":{"id":"lj8gLGtXDftn"}},{"cell_type":"code","source":["#move to the folder where the scraped data is on your device\n","from google.colab import drive\n","drive.mount('/content/drive')\n","%cd /content/drive/MyDrive/Data/scraped_data"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rdcTXHGS1E2R","executionInfo":{"status":"ok","timestamp":1739127970974,"user_tz":-60,"elapsed":2542,"user":{"displayName":"Peace Azubuogu","userId":"15033178143282790060"}},"outputId":"194bf578-f454-440c-98c5-62dd2d463100"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/MyDrive/Data/scraped_data\n"]}]},{"cell_type":"markdown","source":[],"metadata":{"id":"6OuZyi4k1Ek-"}},{"cell_type":"code","execution_count":2,"metadata":{"id":"9yFremjCDbpB","executionInfo":{"status":"ok","timestamp":1739127976271,"user_tz":-60,"elapsed":5294,"user":{"displayName":"Peace Azubuogu","userId":"15033178143282790060"}}},"outputs":[],"source":["# install dependencies\n","#!pip install requirements.txt\n","#!pip install -qU langchain\n","#!pip install -qU transformers\n","#!pip install -qU langchain-community\n","#!pip install -qU unstructured\n","#!pip install -qU sentence-transformers\n","#!pip install faiss-gpu-cu12\n","#!pip install -qU bitsandbytes"]},{"cell_type":"code","source":["from langchain.document_loaders import DirectoryLoader\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","from langchain.embeddings import HuggingFaceEmbeddings\n","from langchain.vectorstores import FAISS\n","from langchain.chains import RetrievalQA\n","from langchain import HuggingFacePipeline\n","from transformers import pipeline, AutoTokenizer, AutoModelForQuestionAnswering, AutoModelForCausalLM, BitsAndBytesConfig\n","import torch\n"],"metadata":{"id":"ZOpscsrjl7rC","executionInfo":{"status":"ok","timestamp":1739128001216,"user_tz":-60,"elapsed":24942,"user":{"displayName":"Peace Azubuogu","userId":"15033178143282790060"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# ensure the notebook is in the same folder as the data files\n","# load all txt files\n","loader1 = DirectoryLoader(r'/content/drive/MyDrive/Data/scraped_data', glob = '*.txt', show_progress = True)"],"metadata":{"id":"zTUJKTVWrs1M","executionInfo":{"status":"ok","timestamp":1739128001222,"user_tz":-60,"elapsed":46,"user":{"displayName":"Peace Azubuogu","userId":"15033178143282790060"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# get content of txt files\n","docs = loader1.load()\n","len(docs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6RS0rfj7t3oZ","executionInfo":{"status":"ok","timestamp":1739128011357,"user_tz":-60,"elapsed":10154,"user":{"displayName":"Peace Azubuogu","userId":"15033178143282790060"}},"outputId":"432b12db-491b-49ec-f8a6-ad0c5eb769c3"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 79/79 [00:10<00:00,  7.80it/s]\n"]},{"output_type":"execute_result","data":{"text/plain":["79"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["# split the txt files into chunks of 1000 characters and 150 characters overlap\n","text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 150)\n","data = text_splitter.split_documents(docs)"],"metadata":{"id":"nXG_a5VHv_zF","executionInfo":{"status":"ok","timestamp":1739128011389,"user_tz":-60,"elapsed":29,"user":{"displayName":"Peace Azubuogu","userId":"15033178143282790060"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["!nvcc --version\n","!nvidia-smi # to ensure GPU and cuda device is in use"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5M0b_nV5Jkl0","executionInfo":{"status":"ok","timestamp":1739128011630,"user_tz":-60,"elapsed":236,"user":{"displayName":"Peace Azubuogu","userId":"15033178143282790060"}},"outputId":"d62237ab-3345-49bc-a68a-492e598bf498"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["nvcc: NVIDIA (R) Cuda compiler driver\n","Copyright (c) 2005-2024 NVIDIA Corporation\n","Built on Thu_Jun__6_02:18:23_PDT_2024\n","Cuda compilation tools, release 12.5, V12.5.82\n","Build cuda_12.5.r12.5/compiler.34385749_0\n","/bin/bash: line 1: nvidia-smi: command not found\n"]}]},{"cell_type":"code","source":["# embed data sources\n","model_kwargs = {'device': 'cpu'}\n","encode_kwargs = {'normalize_embeddings': False}\n","\n","embeddings = HuggingFaceEmbeddings(\n","    model_name = \"sentence-transformers/all-MiniLM-L6-v2\",\n","    model_kwargs = model_kwargs,\n","    encode_kwargs = encode_kwargs\n",")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MvD01m2CHRaD","executionInfo":{"status":"ok","timestamp":1739128019216,"user_tz":-60,"elapsed":7584,"user":{"displayName":"Peace Azubuogu","userId":"15033178143282790060"}},"outputId":"c2c0ba52-7448-47be-8fed-db42e27ef935"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-8-22a22999a28e>:5: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n","  embeddings = HuggingFaceEmbeddings(\n","/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["# vector store\n","db = FAISS.from_documents(data, embeddings)"],"metadata":{"id":"g614T3ZkOeCk","executionInfo":{"status":"ok","timestamp":1739128041387,"user_tz":-60,"elapsed":22160,"user":{"displayName":"Peace Azubuogu","userId":"15033178143282790060"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["# test the retrieval. It retrieves chunks of data on this\n","question = \"What is the Worldbank Ideas Project?\"\n","search_docs = db.similarity_search(question)\n","print(search_docs[1].page_content)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-lVUy0BYRTqy","executionInfo":{"status":"ok","timestamp":1739128041423,"user_tz":-60,"elapsed":31,"user":{"displayName":"Peace Azubuogu","userId":"15033178143282790060"}},"outputId":"0fd022d3-57d9-4511-d388-e19738539cf1"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["In a message to the opening ceremony of the workshop today read on his behalf by the National Project Coordinator, Mrs Blessing Ogwu, Education Minister,Adamu Adamu,told stakeholders that the essence of the IDEAS project is to address the current deficiencies in the education system that have made a large number of school leavers unemployed,urging Nigerian youths to take full advantage of the opportunities offered by the project.\n","\n","According to the Minister,an estimated 40 Technical colleges in the country, alongside the private sector will benefit from the project which also has a technical teacher training component.\n","\n","The Minister revealed that a Two Hundred Million Dollar credit facility from the world Bank has been approved for the project which implementation will span over a five year period.\n"]}]},{"cell_type":"code","source":["# define LLM model to be used for text generation - falcon 7b instruct\n","model_name = \"tiiuae/falcon-7b-instruct\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n","\n","model1 = AutoModelForQuestionAnswering.from_pretrained(model_name, quantization_config=quantization_config, torch_dtype=torch.bfloat16)\n","#model2 = AutoModelForCausalLM.from_pretrained(model_name, load_in_8bit = True, torch_dtype=torch.bfloat16)"],"metadata":{"id":"0eJLRCezkz0T","colab":{"base_uri":"https://localhost:8080/","height":671},"executionInfo":{"status":"error","timestamp":1739128042059,"user_tz":-60,"elapsed":632,"user":{"displayName":"Peace Azubuogu","userId":"15033178143282790060"}},"outputId":"210bde03-9477-4f40-9f8c-7dac339bee08"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stderr","text":["ERROR:bitsandbytes.cextension:Could not load bitsandbytes native library: /lib/x86_64-linux-gnu/libstdc++.so.6: version `GLIBCXX_3.4.32' not found (required by /usr/local/lib/python3.11/dist-packages/bitsandbytes/libbitsandbytes_cpu.so)\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.11/dist-packages/bitsandbytes/cextension.py\", line 85, in <module>\n","    lib = get_native_library()\n","          ^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/bitsandbytes/cextension.py\", line 72, in get_native_library\n","    dll = ct.cdll.LoadLibrary(str(binary_path))\n","          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/ctypes/__init__.py\", line 454, in LoadLibrary\n","    return self._dlltype(name)\n","           ^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/ctypes/__init__.py\", line 376, in __init__\n","    self._handle = _dlopen(self._name, mode)\n","                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n","OSError: /lib/x86_64-linux-gnu/libstdc++.so.6: version `GLIBCXX_3.4.32' not found (required by /usr/local/lib/python3.11/dist-packages/bitsandbytes/libbitsandbytes_cpu.so)\n","CUDA is required but not available for bitsandbytes. Please consider installing the multi-platform enabled version of bitsandbytes, which is currently a work in progress. Please check currently supported platforms and installation instructions at https://huggingface.co/docs/bitsandbytes/main/en/installation#multi-backend\n"]},{"output_type":"error","ename":"RuntimeError","evalue":"CUDA is required but not available for bitsandbytes. Please consider installing the multi-platform enabled version of bitsandbytes, which is currently a work in progress. Please check currently supported platforms and installation instructions at https://huggingface.co/docs/bitsandbytes/main/en/installation#multi-backend","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-a8b63166de03>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mquantization_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBitsAndBytesConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload_in_8bit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmodel1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForQuestionAnswering\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquantization_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquantization_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbfloat16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;31m#model2 = AutoModelForCausalLM.from_pretrained(model_name, load_in_8bit = True, torch_dtype=torch.bfloat16)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mmodel_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_model_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 564\u001b[0;31m             return model_class.from_pretrained(\n\u001b[0m\u001b[1;32m    565\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m             )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3619\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhf_quantizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3620\u001b[0;31m             hf_quantizer.validate_environment(\n\u001b[0m\u001b[1;32m   3621\u001b[0m                 \u001b[0mtorch_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch_dtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3622\u001b[0m                 \u001b[0mfrom_tf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfrom_tf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/quantizers/quantizer_bnb_8bit.py\u001b[0m in \u001b[0;36mvalidate_environment\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0mbnb_multibackend_is_enabled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mis_bitsandbytes_multi_backend_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0mvalidate_bnb_backend_availability\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraise_exception\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"from_tf\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"from_flax\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/integrations/bitsandbytes.py\u001b[0m in \u001b[0;36mvalidate_bnb_backend_availability\u001b[0;34m(raise_exception)\u001b[0m\n\u001b[1;32m    557\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_bitsandbytes_multi_backend_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_validate_bnb_multi_backend_availability\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraise_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 559\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_validate_bnb_cuda_backend_availability\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraise_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/integrations/bitsandbytes.py\u001b[0m in \u001b[0;36m_validate_bnb_cuda_backend_availability\u001b[0;34m(raise_exception)\u001b[0m\n\u001b[1;32m    535\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mraise_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 537\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA is required but not available for bitsandbytes. Please consider installing the multi-platform enabled version of bitsandbytes, which is currently a work in progress. Please check currently supported platforms and installation instructions at https://huggingface.co/docs/bitsandbytes/main/en/installation#multi-backend"]}]},{"cell_type":"code","source":["generation_config = model1.generation_config\n","generation_config =\n","print(generation_config)"],"metadata":{"id":"iS3u1Rk4G3EX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# pipeline for text generation, initialise separate pipeline for other tasks like speech to text, translatipn, etc.\n","\"\"\"text_generator = pipeline(\n","    \"text-generation\",\n","    model=model2,\n","    tokenizer=tokenizer,\n","    torch_dtype = torch.bfloat16,\n","    device_map = \"auto\",\n","    max_length=500,\n","    do_sample=True,\n","    top_k=10,\n","    num_return_sequences=1,\n","    return_full_text=False,\n","    eos_token_id=tokenizer.eos_token_id\n",")\n","\"\"\"\n"],"metadata":{"id":"6tvVMIjK-Hd4","executionInfo":{"status":"aborted","timestamp":1739128042086,"user_tz":-60,"elapsed":3,"user":{"displayName":"Peace Azubuogu","userId":"15033178143282790060"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["question_answering = pipeline(\n","    model=model1,\n","    tokenizer=tokenizer,\n","    task=\"question-answering\",\n","    torch_dtype = torch.bfloat16,\n","    device_map = \"auto\",\n","    max_new_tokens=1000,\n","    do_sample=True,\n","    top_k=10,\n","    num_return_sequences=1,\n","    return_full_text=False,\n",")"],"metadata":{"id":"WBYsa5S2yxs-","executionInfo":{"status":"aborted","timestamp":1739128042098,"user_tz":-60,"elapsed":3,"user":{"displayName":"Peace Azubuogu","userId":"15033178143282790060"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# instantiate the question answering llm pipeline\n","llm1 = HuggingFacePipeline(pipeline = model1, model_kwargs = {'temperature':0})\n","# instantiate the text generator llm pipeline\n","#llm2 = HuggingFacePipeline(pipeline = text_generator, model_kwargs = {'temperature':0, \"max_length\": 1000})"],"metadata":{"id":"NzXPEShiaqtt","executionInfo":{"status":"aborted","timestamp":1739128042108,"user_tz":-60,"elapsed":73942,"user":{"displayName":"Peace Azubuogu","userId":"15033178143282790060"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# chatbot initial prompt template\n","template = \"\"\" You are the official AI chatbot for the Worldbank Ideas Project that follows instruction extremely well. You will provide lots of specific details to user queries on the Worldbank Ideas Project based on {context}. Please, be respectful and friendly and ensure each user has a pleasant experience and conversation with you. Please be truthful and give direct answers\n","\n","### User:\n","{query}\n","\n","### WiChat: \"\"\"\n"],"metadata":{"id":"S1gWXuMilpg0","executionInfo":{"status":"aborted","timestamp":1739128042117,"user_tz":-60,"elapsed":73947,"user":{"displayName":"Peace Azubuogu","userId":"15033178143282790060"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","query = \"What is World Bank IDEAS Project?\"\n","query_embedded = embeddings.embed_query(query)\n"],"metadata":{"id":"4EZWStQzpKkE","executionInfo":{"status":"aborted","timestamp":1739128042123,"user_tz":-60,"elapsed":73949,"user":{"displayName":"Peace Azubuogu","userId":"15033178143282790060"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# retrieve related text\n","retrieved_docs = db.similarity_search(query=query, k=5)\n","retrieved_docs_text = [doc.page_content for doc in retrieved_docs]  # We only need the text of the documents\n","context = \"\\nExtracted documents:\\n\"\n","context += \"\".join([f\"Document {str(i)}:::\\n\" + doc for i, doc in enumerate(retrieved_docs_text)])\n","\n","final_prompt = template.format(query=query, context=context)\n","print(final_prompt)"],"metadata":{"id":"2exvITvVpD0P","executionInfo":{"status":"aborted","timestamp":1739128042129,"user_tz":-60,"elapsed":73952,"user":{"displayName":"Peace Azubuogu","userId":"15033178143282790060"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from langchain import PromptTemplate,  LLMChain\n","\n","\n","prompt = PromptTemplate(template=final_prompt)\n","\n","llm_chain = LLMChain(prompt=prompt, llm=llm1)\n","\n","\n","print(llm_chain.run(query_embedded))"],"metadata":{"id":"kbGe5ChEmanx","executionInfo":{"status":"aborted","timestamp":1739128042132,"user_tz":-60,"elapsed":73952,"user":{"displayName":"Peace Azubuogu","userId":"15033178143282790060"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"clY2jUev21cf"}},{"cell_type":"code","source":["# retriever\n","#retriever = db.as_retriever()"],"metadata":{"id":"-acZJWyWddwn","executionInfo":{"status":"aborted","timestamp":1739128042135,"user_tz":-60,"elapsed":73953,"user":{"displayName":"Peace Azubuogu","userId":"15033178143282790060"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# 1. Audio to Text\n","#speech_to_text = pipeline(\"automatic-speech-recognition\", model=\"facebook/wav2vec2-base-960h\") # Replace model with your model\n","#audio_file = \"path/to/your/audio.wav\" # Replace with your file\n","#text = speech_to_text(audio_file)[\"text\"]\n","\n","# 2. Translation\n","#translator = pipeline(\"translation_en_to_fr\", model=\"Helsinki-NLP/opus-mt-en-fr\") # Replace with the correct language codes and model\n","#translated_text = translator(text)[0]['translation_text']\n","\n","#print(\"Original Text:\", text)\n","#print(\"Translated Text:\", translated_text)\n"],"metadata":{"id":"03SyYfjVjrRW","executionInfo":{"status":"aborted","timestamp":1739128042139,"user_tz":-60,"elapsed":73954,"user":{"displayName":"Peace Azubuogu","userId":"15033178143282790060"}}},"execution_count":null,"outputs":[]}]}